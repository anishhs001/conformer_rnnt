# -*- coding: utf-8 -*-
"""activation_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5szvlcYRNXKIs5DALBqxufpSNRrfLzC
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from einops import rearrange

# List of Activation functions

class softmax(nn.Module):
    """
    Softmax Activation Function
    """
    def forward(self, x):
        return F.softmax(x)


class logsoftmax(nn.Module):
    """
    Log Softmax activation function
    """
    def forward(self, x):
        return F.log_softmax(x)

class glu(nn.Module):
    """
    Gated Linear Units activation function
    """
    def __init__(self, dim=-1):
        super(glu, self).__init__()
        self.glu = nn.GLU(dim=dim)

    def forward(self, x):
        return self.glu(x)

class celu(nn.Module):
    """
    Continuously Differentiable Exponential Linear Units activation function
    """
    def __init__(self, alpha, inplace: bool = False):
        super(selu, self).__init__()
        self.alpha = nn.Parameter(torch.tensor(1.0))
        self.inplace = inplace

    def forward(self, input):
        return F.celu(input, self.alpha, self.inplace)


class selu(nn.Module):
    """
    """

    def __init__(self, inplace: bool = False):
        super(selu, self).__init__()
        self.inplace = inplace

    def forward(self, x):
        return F.selu(x, self.inplace)


class softmax2d(nn.Module):
    """Applies SoftMax over features to each spatial location.

    When given an image of ``Channels x Height x Width``, it will
    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Returns:
        a Tensor of the same dimension and shape as the input with
        values in the range [0, 1]

    Examples::

        >>> m = nn.Softmax2d()
        >>> # you softmax over the 2nd dimension
        >>> input = torch.randn(2, 3, 12, 13)
        >>> output = m(input)
    """

    def forward(self, x):
        assert x.dim() == 4, 'Softmax2d requires a 4D tensor as input'
        return F.softmax(x, 1, _stacklevel=5)


class sigmoid(nn.Module):
    """
    Sigmoid activation function.
    """
    def forward(self, x):
        return torch.sigmoid(x)


class relu(nn.Module):
    """
    ReLU activation function.
    """
    def forward(self, x):
        return torch.relu(x)


class leakyrelu(nn.Module):
    """
    Leaky ReLU activation function.
    """
    def __init__(self, negative_slope=0.01):
        super(leakyrelu, self).__init__()
        self.negative_slope = negative_slope

    def forward(self, x):
        return F.leaky_relu(x, negative_slope=self.negative_slope)


class gatedglu(nn.Module):
    """
    Gated Linear Unit (GLU) activation function.
    """
    def forward(self, x):
        return F.glu(x, dim=-1)

class gelu(nn.Module):
    """
    Gaussian Error Linear Units activation function.

    """
    def forward(self, x, approximate = 'tanh'):
        return F.gelu(x, approximate = approximate)

class swish(nn.Module):
  """
    Swish activation function.
    """
  def forward(self, x):
    return x * torch.sigmoid(x)

class geglu(nn.Module):
  """
    GEGLU activation function.
    """
  def __init__(self, input_size):
    super(geglu, self).__init__()
    self.input_size = input_size
    self.weight_xW = nn.Parameter(torch.randn((input_size, input_size)))
    self.weight_xV = nn.Parameter(torch.randn((input_size, input_size)))
    self.bias_b = nn.Parameter(torch.randn((input_size,)))
    self.bias_c = nn.Parameter(torch.randn((input_size,)))
    self.beta = nn.Parameter(torch.Tensor([1.0]))


  def forward(self, x):
    out_xW = torch.matmul(x, self.weight_xW)
    out_xV = torch.matmul(x, self.weight_xV)
    out_xW = out_xW + self.bias_b
    out_xV = out_xV + self.bias_c
    gate = nn.GELU(out_xV) + self.beta
    x = out_xW * gate
    return x


class swiglu(nn.Module):
  """
    Swiglu activation function.
    """
  def __init__(self, input_size):
    super(swiglu, self).__init__()
    self.input_size = input_size
    self.weight_xW = nn.Parameter(torch.randn((input_size, input_size)))
    self.weight_xV = nn.Parameter(torch.randn((input_size, input_size)))
    self.bias_b = nn.Parameter(torch.randn((input_size,)))
    self.bias_c = nn.Parameter(torch.randn((input_size,)))
    self.beta = nn.Parameter(torch.Tensor([1.0]))


  def forward(self, x):
    out_xW = torch.matmul(x, self.weight_xW)
    out_xV = torch.matmul(x, self.weight_xV)
    out_xW = out_xW + self.bias_b
    out_xV = out_xV + self.bias_c
    gate = swish(out_xV) + self.beta
    x = out_xW * gate
    return x


class swiglu_variant(nn.Module):
  """
    Swiglu activation function.
    """
  def __init__(self, input_size):
    super(swiglu_variant, self).__init__()
    self.input_size = input_size
    self.weight_xW = nn.Parameter(torch.randn((input_size, input_size)))
    self.bias_b = nn.Parameter(torch.randn((input_size,)))
    self.beta = nn.Parameter(torch.Tensor([1.0]))

  def forward(self, x):
    out_xW = torch.matmul(x, self.weight_xW)
    out_xW = out_xW + self.bias_b
    return x * torch.sigmoid(self.beta * x) + (1 - torch.sigmoid(self.beta * x)) * out_xW


class mish(nn.Module):
  """
    Mish(A Self Regularized Non-Monotonic Activation Function) activation function.
    """
  def forward(self, x):
    return x * torch.tanh(F.softplus(x))


class swishl(nn.Module):
  """
    Swish variant SwishL activation function.
    """
  def __init__(self):
    super(swishl, self).__init__()
    self.beta = nn.Parameter(torch.tensor([1.0]))

  def forward(self, x):
    return x * torch.sigmoid(x + self.beta)


class swishr(nn.Module):
  """
    Swish variant SwishR activation function.
    """
  def __init__(self):
    super(swishr, self).__init__()
    self.beta = nn.Parameter(torch.tensor([1.0]))

  def forward(self, x):
    return x * torch.sigmoid(x - self.beta)


class aptx(nn.Module):
  """
    Alpha Plus Tanh Times activation function.
    """
  def __init__(self):
    super(aptx, self).__init__()
    self.alpha = nn.Parameter(torch.tensor([1.0]))
    self.beta = nn.Parameter(torch.tensor([1.0]))
    self.gamma = nn.Parameter(torch.tensor([0.5]))

  def forward(self, x):
    return (x*self.gamma) * (self.alpha + torch.tanh(self.beta*x))


class sigmaptx(nn.Module):
  """
  Combination of sigmoid and tanhx for an activation function.
  """
  def __init__(self):
    super(sigmaptx, self).__init__()
    self.alpha = nn.Parameter(torch.tensor([1.0]))
    self.beta = nn.Parameter(torch.tensor([1.0]))
    self.gamma = nn.Parameter(torch.tensor([1.0]))

  def forward(self, x):
    return (torch.tanh(self.alpha*x)) * (self.beta + torch.sigmoid(x - self.gamma))