{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJMHBO8zV8nD",
        "outputId": "01583a41-b3b4-4676-e5e4-ab6b32756312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting warp-rnnt\n",
            "  Using cached warp_rnnt-0.7.0.tar.gz (15 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install warp-rnnt\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "from torchaudio.models import RNNT\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchaudio.functional import rnnt_loss\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  py_file_location = '/content/drive/MyDrive/models/'\n",
        "  sys.path.append(py_file_location)\n",
        "from activation_functions import aptx, sigmaptx, gelu, glu, relu\n",
        "from adam_variant import ScaledAdam\n",
        "from attention_mechanisms import MultiHeadAttention, MultiHeadSelfAttention\n",
        "from positional_embedding import absolutepositionalembedding, rotarypositionalembedding\n",
        "from decoders import DecoderRNNT\n",
        "# from warp_rnnt import rnnt_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def calc_same_padding(kernel_size):\n",
        "    pad = kernel_size // 2\n",
        "    return (pad, pad - (kernel_size + 1) % 2)\n",
        "\n",
        "# helper classes\n",
        "class DepthWiseConv1d(nn.Module):\n",
        "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups = chan_in)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, self.padding)\n",
        "        return self.conv(x)\n",
        "\n",
        "# attention, feedforward, and conv module\n",
        "\n",
        "class Scale(nn.Module):\n",
        "    def __init__(self, scale, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) * self.scale\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward_Horizontal(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mult = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            sigmaptx(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class FeedForward_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mult = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            aptx(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConformerConvModule_Horizontal(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        expansion_factor = 2,\n",
        "        kernel_size = 31,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim * expansion_factor\n",
        "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            Rearrange('b n c -> b c n'),\n",
        "            nn.Conv1d(dim, inner_dim, 1),\n",
        "            gelu(),\n",
        "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding),\n",
        "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
        "            sigmaptx(),\n",
        "            nn.Conv1d(inner_dim, dim, 1),\n",
        "            Rearrange('b c n -> b n c'),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ConformerConvModule_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        expansion_factor = 2,\n",
        "        kernel_size = 31,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim * expansion_factor\n",
        "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            Rearrange('b n c -> b c n'),\n",
        "            nn.Conv1d(dim, inner_dim * 2, 1),\n",
        "            glu(dim = 1),\n",
        "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding),\n",
        "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
        "            aptx(),\n",
        "            nn.Conv1d(inner_dim, dim, 1),\n",
        "            Rearrange('b c n -> b n c'),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Conformer Block\n",
        "\n",
        "class ConformerBlock_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 8,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ff1 = FeedForward_Vertical(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "        self.position = rotarypositionalembedding(d_model = dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, linear_bias = False, include_local_attention = True, local_attention_window = 9, local_attention_dim_vertical = True)\n",
        "        self.conv = ConformerConvModule_Vertical(dim = dim, causal = conv_causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout)\n",
        "        self.ff2 = FeedForward_Vertical(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "\n",
        "        self.attn = PreNorm(dim, self.attn)\n",
        "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
        "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
        "\n",
        "        self.post_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        x = self.ff1(x) + x\n",
        "        x = self.position(x)\n",
        "        x = self.attn(x, mask = mask) + x\n",
        "        x = self.conv(x) + x\n",
        "        x = self.ff2(x) + x\n",
        "        x = self.post_norm(x)\n",
        "        return x\n",
        "\n",
        "class ConformerBlock_Horizontal(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ff1 = FeedForward_Horizontal(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "        self.position = absolutepositionalembedding(d_model = dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, linear_bias = True, include_local_attention = True, local_attention_window = 9, local_attention_dim_vertical = True)\n",
        "        self.conv = ConformerConvModule_Horizontal(dim = dim, causal = conv_causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout)\n",
        "        self.ff2 = FeedForward_Horizontal(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "\n",
        "        self.attn = PreNorm(dim, self.attn)\n",
        "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
        "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
        "\n",
        "        self.post_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        x = self.ff1(x) + x\n",
        "        x = self.position(x)\n",
        "        x = self.attn(x, mask = mask) + x\n",
        "        x = self.conv(x) + x\n",
        "        x = self.ff2(x) + x\n",
        "        x = self.post_norm(x)\n",
        "        return x\n",
        "\n",
        "# Conformer\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        seq_length,\n",
        "        depth,\n",
        "        output_dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.output_dim = output_dim\n",
        "        self.output_linear = nn.Linear(dim, output_dim, bias = True)\n",
        "        self.layers_vertical = nn.ModuleList([])\n",
        "        self.layers_horizontal = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(int(depth/2)):\n",
        "            self.layers_vertical.append(ConformerBlock_Vertical(\n",
        "                dim = dim,\n",
        "                dim_head = dim_head,\n",
        "                heads = heads,\n",
        "                ff_mult = ff_mult,\n",
        "                conv_expansion_factor = conv_expansion_factor,\n",
        "                conv_kernel_size = conv_kernel_size,\n",
        "                conv_causal = conv_causal\n",
        "\n",
        "            ))\n",
        "            self.layers_horizontal.append(ConformerBlock_Horizontal(\n",
        "                dim = seq_length,\n",
        "                dim_head = dim_head,\n",
        "                heads = heads,\n",
        "                ff_mult = ff_mult,\n",
        "                conv_expansion_factor = conv_expansion_factor,\n",
        "                conv_kernel_size = conv_kernel_size,\n",
        "                conv_causal = conv_causal\n",
        "\n",
        "            ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_vertical = x\n",
        "        x_horizontal = x.transpose(-2, -1)\n",
        "        for block in self.layers_vertical:\n",
        "            x_vertical = block(x_vertical)\n",
        "        for block in self.layers_horizontal:\n",
        "            x_horizontal = block(x_horizontal)\n",
        "        x_horizontal = x_horizontal.transpose(-2, -1)\n",
        "        shape = x_vertical.shape\n",
        "        assert x_vertical.shape == x_horizontal.shape, \"Input tensors must have the same shape\"\n",
        "\n",
        "        # Define dynamic weights as learnable parameters with the same shape as the inputs\n",
        "        weight1 = nn.Parameter(torch.randn(*shape), requires_grad=True)\n",
        "        weight2 = nn.Parameter(torch.randn(*shape), requires_grad=True)\n",
        "\n",
        "        # Compute the weighted sum\n",
        "        weighted_sum = weight1 * x_vertical + weight2 * x_horizontal\n",
        "        # Linear layer to get it in the output dim\n",
        "        output = self.output_linear(weighted_sum)\n",
        "        return output\n",
        "\n",
        "\n",
        "# JointNet to use the the decoding and transducer part of RNNT\n",
        "#Imporved upon the code in https://github.com/ZhengkunTian/rnn-transducer/blob/master/rnnt/ for the Base Encoders, Decoders and the overall transducer\n",
        "class JointNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size):\n",
        "        super(JointNet, self).__init__()\n",
        "        self.forward_layer = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.project_layer = nn.Linear(hidden_size, vocab_size, bias=True)\n",
        "\n",
        "    def forward(self, enc_state, dec_state):\n",
        "        if enc_state.dim() == 3 and dec_state.dim() == 3:\n",
        "            dec_state = dec_state.unsqueeze(1)\n",
        "            enc_state = enc_state.unsqueeze(2)\n",
        "            t = enc_state.size(1)\n",
        "            u = dec_state.size(2)\n",
        "            enc_state = enc_state.repeat([1, 1, u, 1])\n",
        "            dec_state = dec_state.repeat([1, t, 1, 1])\n",
        "        else:\n",
        "            assert enc_state.dim() == dec_state.dim()\n",
        "\n",
        "        concat_state = torch.cat((enc_state, dec_state), dim=-1)\n",
        "        outputs = self.forward_layer(concat_state)\n",
        "        outputs = self.tanh(outputs)\n",
        "        outputs = self.project_layer(outputs)\n",
        "        outputs = outputs.mean(dim=2)\n",
        "        # outputs = F.log_softmax(outputs, dim=-1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Conformer-RNNT Model\n",
        "class ConformerRNNT(nn.Module):\n",
        "    def __init__(self, input_dim, seq_len, num_enc_layers, conv_kernel_size, hidden_dim, output_dim, num_dec_layers, conv_dropout=0.1, enc_has_cont_val = True, share_embedding = True):\n",
        "        super(ConformerRNNT, self).__init__()\n",
        "        self.encoder = Conformer(dim = input_dim, seq_length = seq_length, depth = num_enc_layers, output_dim = output_dim, conv_kernel_size = conv_kernel_size, conv_dropout = conv_dropout)\n",
        "        self.decoder = DecoderRNNT(input_dim = output_dim, hidden_dim = hidden_dim, output_dim = output_dim, num_layers = num_dec_layers, enc_has_cont_val = enc_has_cont_val)\n",
        "        self.joint = JointNet(\n",
        "            input_size=2*output_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            vocab_size=output_dim\n",
        "        )\n",
        "        if share_embedding and not enc_has_cont_val:\n",
        "            assert self.decoder.embedding.weight.size() == self.joint.project_layer.weight.size(), '%d != %d' % (self.decoder.embedding.weight.size(1),  self.joint.project_layer.weight.size(1))\n",
        "            self.joint.project_layer.weight = self.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs, targets, inputs_length = None, targets_length = None):\n",
        "        enc_state = self.encoder(inputs)\n",
        "        dec_state, _ = self.decoder(targets, targets_length)\n",
        "        output = self.joint(enc_state, dec_state)\n",
        "        return output\n",
        "\n",
        "    def recognize(self, inputs, inputs_length):\n",
        "        batch_size = inputs.size(0)\n",
        "        enc_states = self.encoder(inputs, inputs_length)\n",
        "        zero_token = torch.LongTensor([[0]])\n",
        "        if inputs.is_cuda:\n",
        "            zero_token = zero_token.cuda()\n",
        "\n",
        "        def decode(enc_state, lengths):\n",
        "            token_list = []\n",
        "            dec_state, hidden = self.decoder(zero_token)\n",
        "            for t in range(lengths):\n",
        "                logits = self.joint(enc_state[t].view(-1), dec_state.view(-1))\n",
        "                out = F.softmax(logits, dim=0).detach()\n",
        "                pred = torch.argmax(out, dim=0)\n",
        "                pred = int(pred.item())\n",
        "                if pred != 0:\n",
        "                    token_list.append(pred)\n",
        "                    token = torch.LongTensor([[pred]])\n",
        "                    if enc_state.is_cuda:\n",
        "                        token = token.cuda()\n",
        "                    dec_state, hidden = self.decoder(token, hidden=hidden)\n",
        "            return token_list\n",
        "        results = []\n",
        "        for i in range(batch_size):\n",
        "            decoded_seq = decode(enc_states[i], inputs_length[i])\n",
        "            results.append(decoded_seq)\n",
        "        return results"
      ],
      "metadata": {
        "id": "wXLMwizTOYsw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "seq_length = 20\n",
        "input_dim = 128\n",
        "output_dim = 256\n",
        "hidden_dim = 512\n",
        "num_enc_layers = 16\n",
        "num_heads = 8\n",
        "ff_dim = 2048\n",
        "conv_kernel_size = 8\n",
        "num_dec_layers = 16\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "# Create dummy input data\n",
        "inputs = torch.randn(batch_size, seq_length, input_dim, requires_grad = True)\n",
        "\n",
        "# Initialize the model\n",
        "model = ConformerRNNT(input_dim, seq_length, num_enc_layers, conv_kernel_size, hidden_dim, output_dim, num_dec_layers)\n",
        "\n",
        "# Define the optimizer\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "optimizer = ScaledAdam(model.parameters(), lr = 0.00001)\n",
        "\n",
        "targets = torch.randn(batch_size, seq_length, output_dim)  # (batch, target_seq_len)\n",
        "input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long)  # Input lengths (all same in this example)\n",
        "target_lengths = torch.randint(5, output_dim, (batch_size,), dtype=torch.long)  # Random target lengths\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    output = model(inputs, targets)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization step\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I25xLrYP1Zjn",
        "outputId": "0505a2be-206a-400b-9e13-2e13e7dbf449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.0281822681427002\n",
            "Epoch [2/100], Loss: 1.0243932008743286\n",
            "Epoch [3/100], Loss: 1.078877329826355\n",
            "Epoch [4/100], Loss: 1.029217004776001\n",
            "Epoch [5/100], Loss: 1.0327353477478027\n",
            "Epoch [6/100], Loss: 1.0327922105789185\n",
            "Epoch [7/100], Loss: 1.0366309881210327\n",
            "Epoch [8/100], Loss: 1.027967095375061\n",
            "Epoch [9/100], Loss: 1.0295909643173218\n",
            "Epoch [10/100], Loss: 1.023797869682312\n",
            "Epoch [11/100], Loss: 1.0262619256973267\n",
            "Epoch [12/100], Loss: 1.0228439569473267\n",
            "Epoch [13/100], Loss: 2.1144676208496094\n",
            "Epoch [14/100], Loss: 1.7771921157836914\n",
            "Epoch [15/100], Loss: 1.0769721269607544\n",
            "Epoch [16/100], Loss: 1.0611263513565063\n",
            "Epoch [17/100], Loss: 1.04413640499115\n",
            "Epoch [18/100], Loss: 1.0298364162445068\n",
            "Epoch [19/100], Loss: 1.0333975553512573\n",
            "Epoch [20/100], Loss: 1.0369511842727661\n",
            "Epoch [21/100], Loss: 1.0311294794082642\n",
            "Epoch [22/100], Loss: 1.0752931833267212\n",
            "Epoch [23/100], Loss: 1.1231662034988403\n",
            "Epoch [24/100], Loss: 1.113823413848877\n",
            "Epoch [25/100], Loss: 1.5344722270965576\n",
            "Epoch [26/100], Loss: 1.0026254653930664\n",
            "Epoch [27/100], Loss: 1.0050898790359497\n",
            "Epoch [28/100], Loss: 1.0008410215377808\n",
            "Epoch [29/100], Loss: 1.0028877258300781\n",
            "Epoch [30/100], Loss: 1.0032438039779663\n",
            "Epoch [31/100], Loss: 1.0039688348770142\n",
            "Epoch [32/100], Loss: 0.9987334609031677\n",
            "Epoch [33/100], Loss: 0.997344434261322\n",
            "Epoch [34/100], Loss: 0.9971985816955566\n",
            "Epoch [35/100], Loss: 0.9974703192710876\n",
            "Epoch [36/100], Loss: 0.9982037544250488\n",
            "Epoch [37/100], Loss: 0.9973791241645813\n",
            "Epoch [38/100], Loss: 0.9974576830863953\n",
            "Epoch [39/100], Loss: 0.9968349933624268\n",
            "Epoch [40/100], Loss: 0.9966471791267395\n",
            "Epoch [41/100], Loss: 0.9960924386978149\n",
            "Epoch [42/100], Loss: 0.9952305555343628\n",
            "Epoch [43/100], Loss: 0.9951193332672119\n",
            "Epoch [44/100], Loss: 0.9942341446876526\n",
            "Epoch [45/100], Loss: 0.9935793280601501\n",
            "Epoch [46/100], Loss: 0.9931226968765259\n",
            "Epoch [47/100], Loss: 0.9928484559059143\n",
            "Epoch [48/100], Loss: 0.9927659034729004\n",
            "Epoch [49/100], Loss: 0.9925267100334167\n",
            "Epoch [50/100], Loss: 0.9922885894775391\n",
            "Epoch [51/100], Loss: 0.9924752116203308\n",
            "Epoch [52/100], Loss: 0.9923690557479858\n",
            "Epoch [53/100], Loss: 0.9924272298812866\n",
            "Epoch [54/100], Loss: 0.9926695227622986\n",
            "Epoch [55/100], Loss: 0.9925739765167236\n",
            "Epoch [56/100], Loss: 0.9923714399337769\n",
            "Epoch [57/100], Loss: 0.992229163646698\n",
            "Epoch [58/100], Loss: 0.9922422766685486\n",
            "Epoch [59/100], Loss: 0.992419958114624\n",
            "Epoch [60/100], Loss: 0.9920970797538757\n",
            "Epoch [61/100], Loss: 0.992160975933075\n",
            "Epoch [62/100], Loss: 0.9922179579734802\n",
            "Epoch [63/100], Loss: 0.992021381855011\n",
            "Epoch [64/100], Loss: 0.9922868609428406\n",
            "Epoch [65/100], Loss: 0.9920330047607422\n",
            "Epoch [66/100], Loss: 0.9922193884849548\n",
            "Epoch [67/100], Loss: 0.9921737909317017\n",
            "Epoch [68/100], Loss: 0.9919587969779968\n",
            "Epoch [69/100], Loss: 0.9919774532318115\n",
            "Epoch [70/100], Loss: 0.9918664693832397\n",
            "Epoch [71/100], Loss: 0.9918537139892578\n",
            "Epoch [72/100], Loss: 0.9919040203094482\n",
            "Epoch [73/100], Loss: 0.991890549659729\n"
          ]
        }
      ]
    }
  ]
}