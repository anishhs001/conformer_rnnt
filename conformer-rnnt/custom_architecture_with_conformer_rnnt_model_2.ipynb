{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJMHBO8zV8nD",
        "outputId": "d9217aa9-b989-418d-833b-8abcaab5e688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "from torchaudio.models import RNNT\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchaudio.functional import rnnt_loss\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  py_file_location = '/content/drive/MyDrive/models/'\n",
        "  sys.path.append(py_file_location)\n",
        "from activation_functions import aptx, sigmaptx, glu\n",
        "from adam_variant import ScaledAdam\n",
        "from attention_mechanisms import MultiHeadAttention, MultiHeadSelfAttention\n",
        "from positional_embedding import absolutepositionalembedding, rotarypositionalembedding\n",
        "from decoders import DecoderRNNT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def calc_same_padding(kernel_size):\n",
        "    pad = kernel_size // 2\n",
        "    return (pad, pad - (kernel_size + 1) % 2)\n",
        "\n",
        "# helper classes\n",
        "class DepthWiseConv1d(nn.Module):\n",
        "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups = chan_in)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, self.padding)\n",
        "        return self.conv(x)\n",
        "\n",
        "# attention, feedforward, and conv module\n",
        "\n",
        "class Scale(nn.Module):\n",
        "    def __init__(self, scale, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) * self.scale\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward_Horizontal(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mult = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            sigmaptx(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class FeedForward_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mult = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            aptx(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConformerConvModule_Horizontal_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        expansion_factor = 2,\n",
        "        kernel_size = 31,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim * expansion_factor\n",
        "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            Rearrange('b n c -> b c n'),\n",
        "            nn.Conv1d(dim, inner_dim * 2, 1),\n",
        "            glu(dim = 1),\n",
        "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding),\n",
        "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
        "            aptx(),\n",
        "            nn.Conv1d(inner_dim, dim, 1),\n",
        "            Rearrange('b c n -> b n c'),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Conformer Block\n",
        "\n",
        "class ConformerBlock_Horizontal_Vertical(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ff1 = FeedForward_Vertical(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "        self.position = rotarypositionalembedding(d_model = dim)\n",
        "        self.attn1 = MultiHeadSelfAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, linear_bias = False, include_local_attention = True, local_attention_window = 3, local_attention_dim_vertical = True)\n",
        "        self.attn2 = MultiHeadSelfAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, linear_bias = True, include_local_attention = True, local_attention_window = 3, local_attention_dim_vertical = False)\n",
        "        self.conv = ConformerConvModule_Horizontal_Vertical(dim = dim, causal = conv_causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout)\n",
        "        self.ff2 = FeedForward_Horizontal(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "\n",
        "        self.attn1 = PreNorm(dim, self.attn1)\n",
        "        self.attn2 = PreNorm(dim, self.attn2)\n",
        "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
        "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
        "\n",
        "        self.post_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        x = self.ff1(x) + x\n",
        "        x = self.position(x)\n",
        "        x = self.attn1(x, mask = mask) + x\n",
        "        x = self.attn2(x, mask = mask) + x\n",
        "        x = self.conv(x) + x\n",
        "        x = self.ff2(x) + x\n",
        "        x = self.post_norm(x)\n",
        "        return x\n",
        "\n",
        "# Conformer\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(ConformerBlock_Horizontal_Vertical(\n",
        "                dim = dim,\n",
        "                dim_head = dim_head,\n",
        "                heads = heads,\n",
        "                ff_mult = ff_mult,\n",
        "                conv_expansion_factor = conv_expansion_factor,\n",
        "                conv_kernel_size = conv_kernel_size,\n",
        "                conv_causal = conv_causal\n",
        "\n",
        "            ))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.layers:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Conformer-RNNT Model\n",
        "class ConformerRNNT(nn.Module):\n",
        "    def __init__(self, input_dim, num_enc_layers, conv_kernel_size, conv_dropout=0.1):\n",
        "        super(ConformerRNNT, self).__init__()\n",
        "        self.encoder = Conformer(dim = input_dim, depth = num_enc_layers, conv_kernel_size = conv_kernel_size, conv_dropout = conv_dropout)\n",
        "        self.rnnt = DecoderRNNT(num_classes = input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output, dec_hidden_states = self.rnnt(enc_output)\n",
        "        return dec_output"
      ],
      "metadata": {
        "id": "wXLMwizTOYsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "seq_length = 20\n",
        "input_dim = 128\n",
        "output_dim = 128\n",
        "model_dim = 256\n",
        "num_enc_layers = 4\n",
        "num_heads = 8\n",
        "ff_dim = 2048\n",
        "conv_kernel_size = 31\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "# Create dummy input data\n",
        "inputs = torch.randn(batch_size, seq_length, input_dim, requires_grad = True)\n",
        "\n",
        "# Initialize the model\n",
        "model = ConformerRNNT(input_dim, num_enc_layers, conv_kernel_size)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "# optimizer = ScaledAdam(model.parameters(), lr = 0.00001)\n",
        "\n",
        "targets = torch.randn(batch_size, seq_length, output_dim)  # (batch, target_seq_len)\n",
        "input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long)  # Input lengths (all same in this example)\n",
        "target_lengths = torch.randint(5, output_dim, (batch_size,), dtype=torch.long)  # Random target lengths\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    output = model(inputs)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization step\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qozlWlH-KsSg",
        "outputId": "38d8b113-1e8e-40c2-be78-2989f1555e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 1.0054991245269775\n",
            "Epoch [2/25], Loss: 1.0010545253753662\n",
            "Epoch [3/25], Loss: 0.9938554763793945\n",
            "Epoch [4/25], Loss: 1.0127694606781006\n",
            "Epoch [5/25], Loss: 0.9856674075126648\n",
            "Epoch [6/25], Loss: 0.9850400686264038\n",
            "Epoch [7/25], Loss: 0.9800266027450562\n",
            "Epoch [8/25], Loss: 0.9746753573417664\n",
            "Epoch [9/25], Loss: 0.9700107574462891\n",
            "Epoch [10/25], Loss: 0.9653409123420715\n",
            "Epoch [11/25], Loss: 0.9607140421867371\n",
            "Epoch [12/25], Loss: 0.9560474157333374\n",
            "Epoch [13/25], Loss: 0.9524437189102173\n",
            "Epoch [14/25], Loss: 0.9498961567878723\n",
            "Epoch [15/25], Loss: 0.9459260702133179\n",
            "Epoch [16/25], Loss: 0.9473991394042969\n",
            "Epoch [17/25], Loss: 0.9413295984268188\n",
            "Epoch [18/25], Loss: 0.9383352398872375\n",
            "Epoch [19/25], Loss: 0.934430718421936\n",
            "Epoch [20/25], Loss: 0.9408702850341797\n",
            "Epoch [21/25], Loss: 0.9322853088378906\n",
            "Epoch [22/25], Loss: 1.0245431661605835\n",
            "Epoch [23/25], Loss: 0.978649914264679\n",
            "Epoch [24/25], Loss: 0.9788764715194702\n",
            "Epoch [25/25], Loss: 0.9744836091995239\n"
          ]
        }
      ]
    }
  ]
}